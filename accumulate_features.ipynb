{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tokenizers\n",
    "import bottleneck_llm\n",
    "import os\n",
    "import sae\n",
    "import tqdm\n",
    "config = {\n",
    "    \"learning_rate\": 2e-3,\n",
    "    \"sae_learning_rate\": 5e-5,\n",
    "    \"model_embedding_layer\": 6,\n",
    "    \"eval_interval\": 500,\n",
    "    \"max_iters\": 60000, \n",
    "    \"H\": 32, # hidden dimension size\n",
    "    \"B\": 128,\n",
    "    \"T\": 256,\n",
    "    \"C\": 256,\n",
    "    \"feedforward_factor\": 3,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 12,\n",
    "    \"sae_size\": 2**14,\n",
    "    \"sae_location\": 6,\n",
    "    \"sae_topk\": 20,\n",
    "    \"sae_r2_lambda\": 2,\n",
    "    \"sae_mse_lambda\": 0,\n",
    "\n",
    "    \"vocab_size\": 2**13,\n",
    "    'expt_name': 'e2e_sae_1',\n",
    "    \"git_hash\": os.popen(\"git rev-parse HEAD\").read().strip()\n",
    "}\n",
    "\n",
    "for k,v in config.items():\n",
    "    locals()[k] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('tiny-stories-train.pt', map_location='cuda')\n",
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_by_index(split, ix):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    x = torch.stack([data[i:i+T] for i in ix]) # random sequences\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in ix]) # next character for each random sequence\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = sae.TopKSparseAutoencoder(C, sae_size, sae_topk)\n",
    "b = bottleneck_llm.BottleNeckGPT(\n",
    "    B=B,\n",
    "    T=T,\n",
    "    C=C,\n",
    "    n_heads=n_heads,\n",
    "    H=H,\n",
    "    n_layers = n_layers,\n",
    "    bottleneck_model=autoencoder,\n",
    "    bottleneck_location=sae_location,\n",
    "    vocab_size=vocab_size,\n",
    "    )\n",
    "\n",
    "b.load_state_dict(torch.load(\"e2e_sae_1/joint_composed.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on chunk 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 1000/1000 [00:52<00:00, 19.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on chunk 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 1000/1000 [00:52<00:00, 19.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on chunk 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 1000/1000 [00:53<00:00, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on chunk 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 1000/1000 [00:53<00:00, 18.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on chunk 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 1000/1000 [00:53<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on chunk 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 1000/1000 [00:52<00:00, 19.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on chunk 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 1000/1000 [00:53<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 1000/1000 [00:52<00:00, 19.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on chunk 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 1000/1000 [00:51<00:00, 19.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch: 100%|██████████| 1000/1000 [00:52<00:00, 19.08it/s]\n"
     ]
    }
   ],
   "source": [
    "batches_per_chunk = 1000\n",
    "tokens_per_batch = T*B\n",
    "\n",
    "def write_encoded_data():\n",
    "    with torch.no_grad():\n",
    "        b.eval()\n",
    "        b.bottleneck_model.eval()\n",
    "\n",
    "        \n",
    "        for chunk in range(10):\n",
    "            print('on chunk', chunk)\n",
    "            accum_idxs = []\n",
    "            accum_values = []\n",
    "\n",
    "            chunk_start = chunk * batches_per_chunk * tokens_per_batch\n",
    "            for i in tqdm.tqdm(range(batches_per_chunk), desc='batch'):  \n",
    "                start = T*B * i + chunk_start\n",
    "                end = T*B * (i+1) + chunk_start\n",
    "\n",
    "                index = torch.arange(start, end, T)\n",
    "                x, y = get_batch_by_index('train', index)\n",
    "                ret = b.forward(x, targets=None, bottleneck_early_stop=True)\n",
    "                sparse_idxs = ret['bm_results']['topk_idxs'].to(torch.int16).to('cpu')\n",
    "                sparse_values = ret['bm_results']['topk_values'].to(torch.float16).to('cpu')\n",
    "                accum_idxs.append(sparse_idxs)\n",
    "                accum_values.append(sparse_values)\n",
    "\n",
    "            cat_idxs = torch.cat(accum_idxs)\n",
    "            cat_values = torch.cat(accum_values)\n",
    "            torch.save(cat_idxs, f'{expt_name}/encoded/accum_idxs-{chunk:03d}.pt')\n",
    "            torch.save(cat_values, f'{expt_name}/encoded/accum_values-{chunk:03d}.pt')\n",
    "# write_encoded_data()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = torch.load(f'{expt_name}/encoded/accum_idxs-000.pt')\n",
    "values = torch.load(f'{expt_name}/encoded/accum_values-000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2733, 10675,  4032,  5123, 13338, 14089, 11776, 14339, 10797,  5472,\n",
      "           17, 14080,  2563,  6662,  5900, 14191,  7483,  6661,  8251,  2184],\n",
      "       dtype=torch.int16)\n",
      "tensor([19.2344,  6.7383,  6.6094,  5.8867,  5.8672,  5.4062,  4.7773,  3.0020,\n",
      "         2.9258,  2.6289,  2.0625,  1.5215,  0.7817,  0.7651,  0.4709,  0.4026,\n",
      "         0.3823,  0.3687,  0.3574,  0.1311], dtype=torch.float16)\n",
      "torch.Size([128000, 256, 20])\n"
     ]
    }
   ],
   "source": [
    "print (idxs[0][0])\n",
    "print (values[0][0])\n",
    "print(idxs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 321, 5321) ->  76059849\n",
      "(    0,     0,     1)                    1\n",
      "(    0,     1,     0)                32768\n",
      "(    1,     0,     0)             32768000\n",
      "(    0,     0, 32767)                32767\n",
      "(    0,   999,     0)             32735232\n",
      "(    9,     0,     0)            294912000\n"
     ]
    }
   ],
   "source": [
    "def global_token_index(chunk_no, batch_no, token_no):\n",
    "    chunk_start = tokens_per_batch * batches_per_chunk * chunk_no \n",
    "    batch_start = tokens_per_batch * batch_no\n",
    "    token_start = token_no\n",
    "    return chunk_start + batch_start + token_start\n",
    "\n",
    "print('(2, 321, 5321) -> ', global_token_index(2, 321, 5321))\n",
    "for arg in [(0, 0, 1), (0, 1, 0), (1, 0, 0), (0, 0, 32767), (0, 999, 0), (9, 0, 0)]:\n",
    "    print(f'({arg[0]:5d}, {arg[1]:5d}, {arg[2]:5d}) {str(global_token_index(*arg)).rjust(20)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mommy and said, \"Mommy, look at the big balloon! It makes me so happy!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    \"./tiny-stories-bpe-vocab.json\", \n",
    "    \"./tiny-stories-bpe-merges.txt\"\n",
    ")\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "def decode(encoded_text):\n",
    "    return tokenizer.decode(encoded_text)\n",
    "\n",
    "def get_text_from_global_index(chunk_no, batch_no, token_no, context_size=10):\n",
    "    idx = global_token_index(chunk_no, batch_no, token_no)\n",
    "    return decode(train_data[idx-context_size:idx+context_size].tolist())\n",
    "\n",
    "print(get_text_from_global_index(2, 3213, 40))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
