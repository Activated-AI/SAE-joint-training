{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.24.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if not 'RAN_PIP' in locals():\n",
    "    !pip install tokenizers\n",
    "    RAN_PIP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tokenizers\n",
    "import llm\n",
    "import os\n",
    "import sae\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# expt_name = 'e2e_sae_1'\n",
    "expt_name = 'vanilla_split_llm_sae'\n",
    "expt_dir = f'experiments/{expt_name}'\n",
    "\n",
    "def loadconfig():\n",
    "    global config\n",
    "    config = json.load(open(f\"experiments/{expt_name}/config.json\"))\n",
    "    for k,v in config.items():\n",
    "        globals()[k] = v\n",
    "\n",
    "loadconfig()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('tiny-stories-train.pt', map_location='cuda')\n",
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_by_index(split, ix):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    x = torch.stack([data[i:i+T] for i in ix]) # random sequences\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in ix]) # next character for each random sequence\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading separate LLM and SAE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "llm_args = ['B', 'T', 'C', 'n_heads', 'H', 'n_layers', 'vocab_size']\n",
    "llm_kwargs = {k: globals()[k] for k in llm_args}\n",
    "\n",
    "autoencoder = sae.TopKSparseAutoencoder(C, sae_size, sae_topk)\n",
    "if config.get(\"separate_llm\", False):\n",
    "    print(\"Loading separate LLM and SAE\")\n",
    "    gpt = llm.GPT(**llm_kwargs)\n",
    "    autoencoder.load_state_dict(torch.load(f'{expt_dir}/sae.pt'))\n",
    "    def get_latents(tokens):\n",
    "        llm_out = gpt.forward(tokens, targets=None, stop_at_layer=sae_location)\n",
    "        residuals = llm_out['residuals']\n",
    "        sae_out = autoencoder(residuals)\n",
    "        #if random.random() < 0.1:\n",
    "        #    print(\"r2\", sae_out['mean_r2'])\n",
    "        #    print(\"top idx\", sae_out['topk_idxs'][0,0])\n",
    "        sparse_idxs = sae_out['topk_idxs']\n",
    "        sparse_values = sae_out['topk_values']\n",
    "        return sparse_idxs, sparse_values\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Loading e2e LLM and SAE\")\n",
    "    gpt = llm.BottleNeckGPT(\n",
    "        bottleneck_model=autoencoder,\n",
    "        bottleneck_location=sae_location,\n",
    "        **llm_kwargs\n",
    "    )\n",
    "    def get_latents(tokens):\n",
    "        ret = gpt(tokens, targets=None, bottleneck_early_stop=True)\n",
    "        sparse_idxs = ret['bm_results']['topk_idxs'].to(torch.int16)\n",
    "        sparse_values = ret['bm_results']['topk_values'].to(torch.float16)\n",
    "        return sparse_idxs, sparse_values\n",
    "\n",
    "gpt.load_state_dict(torch.load(f'{expt_dir}/gpt.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 4914,  1297,  6193,  ...,    91,  1897,  2058],\n",
       "          [ 4914,  1297,  6193,  ...,  7780,  1382,  2748],\n",
       "          [ 4914,  1297,  6193,  ...,  8518, 13071,  2712],\n",
       "          ...,\n",
       "          [ 1297,  4914, 14637,  ...,  6456,  4910, 14614],\n",
       "          [ 1297,  4914, 14637,  ..., 11958,  9294, 14772],\n",
       "          [ 1297,  4914, 14637,  ...,  4921, 13256,  7421]],\n",
       " \n",
       "         [[ 4914,  1297,  6193,  ...,  5191,   154,  7317],\n",
       "          [ 4914,  1297,  6193,  ..., 11156, 13445, 12563],\n",
       "          [ 4914,  1297,  6193,  ...,   423,  5465,  1337],\n",
       "          ...,\n",
       "          [ 1297,  4914, 14637,  ..., 13413, 10861,  6913],\n",
       "          [ 1297,  4914, 14637,  ...,  1085, 13614,  2320],\n",
       "          [ 1297,  4914, 14637,  ...,  8495,  2574,   154]],\n",
       " \n",
       "         [[ 4914,  1297,  6193,  ...,  6723, 11977,  3891],\n",
       "          [ 1297,  4914,  6193,  ..., 11108,  9328,  1897],\n",
       "          [ 1297,  4914,  6193,  ..., 10828,  1206, 15124],\n",
       "          ...,\n",
       "          [ 1297,  4914, 14637,  ...,  4768,  1276,  3782],\n",
       "          [ 1297,  4914, 14637,  ...,  2159,  5754, 12136],\n",
       "          [ 1297,  4914, 14637,  ..., 14437, 15698,  9102]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 4914,  1297,  6193,  ...,  3861,  2064, 16220],\n",
       "          [ 4914,  1297,  6193,  ...,  5626, 16220,  2141],\n",
       "          [ 4914,  1297,  6193,  ..., 15545,  8441,   434],\n",
       "          ...,\n",
       "          [ 1297, 14637,  4914,  ...,  2506,  8011,  3145],\n",
       "          [ 1297, 14637,  4914,  ..., 11847, 12041,  7372],\n",
       "          [ 1297, 14637,  4914,  ..., 15965, 13090,  3218]],\n",
       " \n",
       "         [[ 4914,  1297,  6193,  ...,  6144,  9409,  3612],\n",
       "          [ 1297,  4914,  6193,  ..., 15965,  2118,  1368],\n",
       "          [ 1297,  4914,  6193,  ...,  6850, 15965,  5212],\n",
       "          ...,\n",
       "          [ 1297,  4914, 14637,  ...,  9648,  9492,  1590],\n",
       "          [ 1297,  4914, 14637,  ..., 12712, 10948,  4979],\n",
       "          [ 1297,  4914, 14637,  ..., 12631, 12112, 13298]],\n",
       " \n",
       "         [[ 4914,  1297,  6193,  ...,  4385,  9377, 14398],\n",
       "          [ 4914,  1297,  6193,  ...,  8936,  7086,  7139],\n",
       "          [ 4914,  1297,  6193,  ..., 15102, 13171, 10637],\n",
       "          ...,\n",
       "          [ 1297,  4914, 14637,  ..., 10194,  6554, 11840],\n",
       "          [ 1297,  4914, 14637,  ...,  1577, 10547,  1419],\n",
       "          [ 1297,  4914, 14637,  ..., 10941,  5620, 12097]]], device='cuda:0'),\n",
       " tensor([[[56.1344, 46.4485, 36.7155,  ...,  3.4265,  3.4125,  3.3753],\n",
       "          [40.6016, 36.2226, 30.4645,  ...,  2.4849,  2.4491,  2.3545],\n",
       "          [40.3040, 36.9468, 29.3717,  ...,  2.3793,  2.2315,  2.2213],\n",
       "          ...,\n",
       "          [40.8233, 23.9310, 20.4973,  ...,  2.9071,  2.7042,  2.6719],\n",
       "          [42.0942, 25.3363, 22.2450,  ...,  1.7204,  1.7183,  1.6177],\n",
       "          [45.3997, 26.9854, 22.0869,  ...,  2.3626,  2.3269,  2.2967]],\n",
       " \n",
       "         [[56.5480, 56.0408, 39.9563,  ...,  1.8256,  1.7956,  1.7897],\n",
       "          [44.5367, 40.7744, 31.0131,  ...,  2.9568,  2.8976,  2.7086],\n",
       "          [45.1573, 45.0222, 31.6099,  ...,  1.9724,  1.9248,  1.7934],\n",
       "          ...,\n",
       "          [38.2208, 26.2288, 21.8375,  ...,  2.1449,  2.1381,  2.0527],\n",
       "          [38.3671, 29.0239, 21.7069,  ...,  1.9687,  1.8883,  1.8689],\n",
       "          [40.1560, 27.4314, 20.4405,  ...,  1.8815,  1.8506,  1.8491]],\n",
       " \n",
       "         [[55.2223, 50.3876, 35.3832,  ...,  3.0327,  3.0051,  2.9876],\n",
       "          [44.4489, 41.5127, 31.0805,  ...,  2.8110,  2.7681,  2.6406],\n",
       "          [40.6330, 36.3424, 29.8560,  ...,  2.0759,  2.0029,  1.9727],\n",
       "          ...,\n",
       "          [39.1061, 28.2347, 20.9506,  ...,  2.7351,  2.6713,  2.6271],\n",
       "          [40.8109, 29.1617, 20.4443,  ...,  2.2122,  2.2044,  2.0558],\n",
       "          [41.2496, 29.0167, 20.9917,  ...,  2.5641,  2.5540,  2.4925]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[56.3552, 53.9068, 38.3708,  ...,  2.0824,  2.0621,  2.0593],\n",
       "          [40.5190, 39.4971, 30.4994,  ...,  2.1046,  2.0845,  2.0681],\n",
       "          [37.4560, 36.8665, 29.3388,  ...,  2.8332,  2.6995,  2.5963],\n",
       "          ...,\n",
       "          [42.8909, 22.8773, 21.2390,  ...,  2.0053,  1.9905,  1.9545],\n",
       "          [43.6134, 23.6375, 23.0695,  ...,  2.1845,  2.1332,  2.1277],\n",
       "          [41.5779, 21.7612, 20.9679,  ...,  2.1775,  2.1748,  2.1684]],\n",
       " \n",
       "         [[59.9186, 57.5686, 39.6610,  ...,  1.8976,  1.8868,  1.8770],\n",
       "          [41.0315, 39.7446, 29.6944,  ...,  1.9660,  1.9381,  1.9271],\n",
       "          [36.5170, 36.2355, 26.8042,  ...,  2.1519,  2.1112,  2.0801],\n",
       "          ...,\n",
       "          [42.9127, 27.0404, 22.0983,  ...,  2.5503,  2.5448,  2.4892],\n",
       "          [41.3517, 28.1967, 23.9586,  ...,  2.1926,  2.1834,  2.0771],\n",
       "          [44.1825, 28.6376, 22.3315,  ...,  2.2149,  2.1356,  2.0567]],\n",
       " \n",
       "         [[57.0246, 53.7407, 36.8532,  ...,  2.0650,  2.0574,  2.0106],\n",
       "          [40.6507, 37.8565, 29.8061,  ...,  2.9110,  2.8919,  2.8907],\n",
       "          [35.6907, 34.0458, 27.2872,  ...,  3.1285,  3.1192,  3.0765],\n",
       "          ...,\n",
       "          [39.5208, 22.1268, 20.8026,  ...,  3.2884,  3.0077,  2.8722],\n",
       "          [40.5378, 22.5794, 20.4945,  ...,  2.1599,  2.0580,  2.0540],\n",
       "          [44.1684, 27.6444, 23.8589,  ...,  3.1246,  2.9848,  2.8643]]],\n",
       "        device='cuda:0', grad_fn=<TopkBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, data.size(0) - T, (B,)) # 4 random locations we can sample from\n",
    "    x = torch.stack([data[i:i+T] for i in ix]) # random sequences\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in ix]) # next character for each random sequence\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# for b in range(B):\n",
    "#     for t in range(T): # for each of the characters in the sample\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b, t]\n",
    "\n",
    "get_latents(xb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding validation data: 100%|██████████| 1428/1428 [01:10<00:00, 20.12it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f'{expt_dir}/encoded', exist_ok=True)\n",
    "\n",
    "def write_encoded_data():\n",
    "    with torch.no_grad():\n",
    "        validation_tokens = val_data.shape[0]\n",
    "        \n",
    "        tokens_per_batch = B*T\n",
    "        num_batches = validation_tokens // tokens_per_batch\n",
    "\n",
    "        accum_idxs = []\n",
    "        accum_values = []\n",
    "\n",
    "        for i in tqdm.tqdm(range(num_batches), desc=f'encoding validation data'):  \n",
    "            start = T*B * i\n",
    "            end = T*B * (i+1) \n",
    "\n",
    "            index = torch.arange(start, end, T)\n",
    "            x, y = get_batch_by_index('test', index)\n",
    "            sparse_idxs, sparse_values = get_latents(x)\n",
    "            accum_idxs.append(sparse_idxs)\n",
    "            accum_values.append(sparse_values)\n",
    "\n",
    "        cat_idxs = torch.cat(accum_idxs)\n",
    "        cat_values = torch.cat(accum_values)\n",
    "        torch.save(cat_idxs.view(-1, sae_topk), f'{expt_dir}/encoded/test_accum_idxs.pt')\n",
    "        torch.save(cat_values.view(-1, sae_topk), f'{expt_dir}/encoded/test_accum_values.pt')\n",
    "write_encoded_data()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = torch.load(f'{expt_name}/encoded/test_accum_idxs.pt')\n",
    "values = torch.load(f'{expt_name}/encoded/test_accum_values.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2733, dtype=torch.int16)\n",
      "tensor(16.8125, dtype=torch.float16)\n",
      "torch.Size([46792704, 20])\n"
     ]
    }
   ],
   "source": [
    "print (idxs[0][0])\n",
    "print (values[0][0])\n",
    "print(idxs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' small', ' had lots of green leaves. One day, a small seed fell from the tree and landed on the')\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    \"./tiny-stories-bpe-vocab.json\", \n",
    "    \"./tiny-stories-bpe-merges.txt\"\n",
    ")\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "def decode(encoded_text):\n",
    "    return tokenizer.decode(encoded_text)\n",
    "\n",
    "def get_text_from_global_index(token_idx, context_size=10):\n",
    "    token = val_data[token_idx].item()\n",
    "    return decode([token]), decode(val_data[token_idx-context_size:token_idx+context_size].tolist())\n",
    "\n",
    "print(get_text_from_global_index(int(24 * 1e6)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
